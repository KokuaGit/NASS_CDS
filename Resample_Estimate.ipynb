{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "1. ロジスティック回帰を用いることで過去手法と比較\n",
    "    - 係数をそのまま使用\n",
    "    - 特徴量をそのまま使用\n",
    "    - 今回の特徴量で\n",
    "1. BMIのグルーピングを最重量のグループかどうかでグルーピング\n",
    "1. 車両の面積を特徴量として入れてみる\n",
    "1. 決定木の可視化\n",
    "1. 親ノードを調整\n",
    "1. ベイジアンネットワークの勉強\n",
    "1. GNNの調査  \n",
    "不要かも\n",
    "1. PLSA  \n",
    "不要かも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import imblearn\n",
    "import tqdm\n",
    "import graphviz\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, ClusterCentroids, NearMiss\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import  confusion_matrix, classification_report, roc_curve, roc_auc_score, auc, accuracy_score, precision_recall_curve, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from IPython import embed\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import HTML\n",
    "from dtreeviz.trees import dtreeviz\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.options.display.float_format = '{:.4g}'.format\n",
    "class pycolor:\n",
    "    BLACK = '\\033[30m'\n",
    "    RED = '\\033[31m'\n",
    "    GREEN = '\\033[32m'\n",
    "    YELLOW = '\\033[33m'\n",
    "    BLUE = '\\033[34m'\n",
    "    PURPLE = '\\033[35m'\n",
    "    CYAN = '\\033[36m'\n",
    "    WHITE = '\\033[37m'\n",
    "    END = '\\033[0m'\n",
    "    BOLD = '\\038[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    INVISIBLE = '\\033[08m'\n",
    "    REVERCE = '\\033[07m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グルーピング用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping(df):\n",
    "    group_list = [['MODELYR', 'MODELG', [1990, 1997, 2001]],\n",
    "                  ['CURBWGT', 'WGTG', [101, 201]], \n",
    "                  ['otvehwgt', 'owgtg', [101, 201]],\n",
    "                  ['AGE', 'AGEG', [60, 70]],\n",
    "                  ['BMI', 'BMIGJP', [18.5, 25, 30, 35, 40]],\n",
    "                  ['PDOF1', 'PDOFG', [35, 145, 215, 325]]\n",
    "                 ]\n",
    "    for glist in group_list:\n",
    "        if glist[0] in list(df.columns):\n",
    "            for i in range(len(glist[2]) + 1):\n",
    "                if i == 0:\n",
    "                    df.loc[df[glist[0]] < glist[2][0], glist[1]] = 0\n",
    "                elif i < len(glist[2]):\n",
    "                    df.loc[(df[glist[0]] >= glist[2][i - 1]) & (df[glist[0]] < glist[2][i]), glist[1]] = i\n",
    "                else:\n",
    "                    df.loc[df[glist[0]] >= glist[2][-1], glist[1]] = i\n",
    "    df.loc[df['PDOFG'] == 4, 'PDOFG'] = 0\n",
    "    #df['POS_DOF'] = 0\n",
    "    #df.loc[(((df['PDOF1'] >= 0) & (df['PDOF1'] <= 110)) | ((df['PDOF1'] >= 340) & (df['PDOF1'] <= 360))) & ((df['SEATLR2'] == 2) & (df['SEATROW2'] == 1)), 'POS_DOF'] = 1\n",
    "    #df.loc[((df['PDOF1'] >= 70) & (df['PDOF1'] <= 200)) & ((df['SEATLR2'] == 2) & (df['SEATROW2'] == 2)), 'POS_DOF'] = 1\n",
    "    #df.loc[((df['PDOF1'] >= 160) & (df['PDOF1'] <= 290)) & ((df['SEATLR2'] == 1) & (df['SEATROW2'] == 2)), 'POS_DOF'] = 1\n",
    "    #df.loc[(((df['PDOF1'] >= 250) & (df['PDOF1'] <= 360)) | ((df['PDOF1'] >= 0) & (df['PDOF1'] <= 20))) & ((df['SEATLR2'] == 1) & (df['SEATROW2'] == 1)), 'POS_DOF'] = 1    \n",
    "    for i in ['MODELYR', 'CURBWGT', 'otvehwgt', 'AGE', 'BMI', 'PDOF1']:\n",
    "        if i in df.columns:\n",
    "            df = df.drop(columns = i, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.path.abspath('__file__'))\n",
    "df = pd.read_csv(os.path.join(path, 'grouped_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データに含まれるカラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプル用数値振り直し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dic = {'MANCOLL': {0:4, 1:5, 2:3, 4:1, 5:0, 6:2},\n",
    "           'BODYG': {0:4, 1:1, 2:2, 3:3, 4:0},\n",
    "           'obodyg': {0:0, 1:1, 2:3, 3:2, 4:4, 5:5}}\n",
    "for i in map_dic.keys():\n",
    "    #count = df[i].isnull().sum()\n",
    "    df[i] = df[i].map(map_dic[i])\n",
    "    #df[i].isnull().sum() - count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不要なカラムをドロップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上下どちらかのセルを選択して実行\n",
    "df = df.drop(columns = 'DVper10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'DVTOTAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'TRAVELSP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NANと同じ意味のデータをNANに置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TRAVELSP'] = df['TRAVELSP'].replace(777, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相関係数の高い特徴量をドロップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全てグルーピング後相関係数に応じて特徴量を削除したデータを作成\n",
    "df_dic  = {}\n",
    "count = 0\n",
    "for i, col1 in enumerate([['MODELG', 'SEATROW2'], ['BAGAVAIL', 'BAGAVOTH']]):\n",
    "    for j, col2 in enumerate([['BODYG', 'obodyg'], ['WGTG',  'owgtg']]):\n",
    "        df_dic[count] = df.drop(columns = col1 + col2)\n",
    "        #df_dic[count] = df.drop(columns = 'POS_DOF')\n",
    "        print(count)\n",
    "        print('remain : ', list(df_dic[count].columns))\n",
    "        print('drop : ', col1 + col2)\n",
    "        count += 1\n",
    "df_dic[count] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#連続量の場合に相関係数に応じて特徴量を削除\n",
    "df_dic  = {}\n",
    "count = 0\n",
    "for i, col1 in enumerate([['MODELYR', 'SEATROW2'], ['BAGAVAIL', 'BAGAVOTH']]):\n",
    "    for j, col2 in enumerate([['BODYG', 'obodyg'], ['CURBWGT',  'otvehwgt']]):\n",
    "        df_dic[count] = df.drop(columns = col1 + col2)\n",
    "        #df_dic[count] = df.drop(columns = 'POS_DOF')\n",
    "        print(count)\n",
    "        print('remain : ', list(df_dic[count].columns))\n",
    "        print('drop : ', col1 + col2)\n",
    "        count += 1\n",
    "df_dic[count] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グルーピング\n",
    "リサンプリングしない場合はここでグルーピングを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 5):\n",
    "    df_dic[i] = grouping(df_dic[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化\n",
    "ランダムフォレストのみなら不要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdcol = ['SPLIMIT', 'TRAVELSP', 'DVTOTAL']\n",
    "for i in range(5):\n",
    "    for j in stdcol:\n",
    "        std = StandardScaler()\n",
    "        df_std = df_dic[i].query('YEAR < 2015')\n",
    "        if j in df_std.columns:\n",
    "            null_val = df_std[j].isnull()\n",
    "            std.fit(df_std.loc[~null_val, [j]])\n",
    "            null_val = df_dic[i][j].isnull()\n",
    "            df_dic[i].loc[~null_val, [j]] = std.transform(df_dic[i].loc[~null_val, [j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中央値で欠損値補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in list(df_dic[i].columns):\n",
    "        df_med = df_dic[i].query('YEAR < 2015')\n",
    "        med = df_med[j].median()\n",
    "        if df_dic[i][j].isnull().any():\n",
    "            print(j, ':', med)\n",
    "        df_dic[i][j] = df_dic[i][j].fillna(med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値補完の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('欠損値数：', df_dic[i].isnull().any().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータと訓練データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "explanatory = {}\n",
    "y_train = df.query('YEAR < 2015')['MAIS']\n",
    "y_train3 = df.query('YEAR < 2015')['MAIS3']\n",
    "y_test = df.query('YEAR == 2015')['MAIS']\n",
    "y_test3 = df.query('YEAR == 2015')['MAIS3']\n",
    "objective = df['MAIS']\n",
    "objective3 = df['MAIS3']\n",
    "for i in range(5):\n",
    "    train[i] = df_dic[i].query('YEAR < 2015').drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    test[i] = df_dic[i].query('YEAR == 2015').drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    explanatory[i] = df_dic[i].drop(columns = ['YEAR', 'MAIS', 'MAIS3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ダミー変数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dic = {}\n",
    "not_dummy = ['YEAR', 'MAIS', 'MAIS3', 'DVTOTAL', 'TRAVELSP']\n",
    "for i in range(5):\n",
    "    dummy_dic[i] = pd.get_dummies(df_dic[i], drop_first = False, columns = [x for x in list(df_dic[i].columns) if x not in not_dummy])\n",
    "#ダミー変数化したデータをテストデータと訓練データに分割\n",
    "train_dummy =  {}\n",
    "test_dummy =  {}\n",
    "for i in range(5):\n",
    "    train_dummy[i] = dummy_dic[i].query('YEAR < 2015').drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    test_dummy[i] = dummy_dic[i].query('YEAR == 2015').drop(columns = ['YEAR', 'MAIS', 'MAIS3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ランダムフォレストで推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_dic = {}\n",
    "gs_result = {}\n",
    "predict = {}#テストデータセットの推定結果\n",
    "pred = {}#訓練データでの推定結果\n",
    "f_imp_dic = {}\n",
    "grid_param = {'max_depth': [5], 'min_samples_split': [2], 'min_samples_leaf': [2], 'n_estimators': [500]}\n",
    "for i in range(5):\n",
    "    forest_dic[i] = GridSearchCV(RandomForestClassifier(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc')\n",
    "    forest_dic[i].fit(train[i], y_train3)\n",
    "    predict[i] = forest_dic[i].predict(test[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test3, predict[i], pos_label = 1)\n",
    "    print('best parameter:', forest_dic[i].best_params_)\n",
    "    print('dic num:', i)\n",
    "    print('test auc:', auc(fpr, tpr))\n",
    "    print('test accuracy:', accuracy_score(y_test3, predict[i]) * 100)\n",
    "    pred[i] = forest_dic[i].predict(train[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_train3, pred[i], pos_label = 1)\n",
    "    print('train auc:', auc(fpr, tpr))\n",
    "    print('train accuracy:', accuracy_score(y_train3, pred[i]) * 100)\n",
    "    gs_result[i] = pd.DataFrame.from_dict(forest_dic[i].cv_results_)\n",
    "    f_imp_dic[i] = pd.DataFrame(index = range(len(train[i].columns)))\n",
    "    imp = forest_dic[i].best_estimator_.feature_importances_\n",
    "    for j, col in enumerate(train[i].columns):\n",
    "        f_imp_dic[i].at[j, 'column'] = col\n",
    "        f_imp_dic[i].at[j, 'importance'] = imp[j]\n",
    "    f_imp_dic[i] = f_imp_dic[i].sort_values('importance', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    f_imp_dic[i].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際のMAIS3+が1で推定結果が0となっているデータの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoge = df.query('YEAR == 2015')\n",
    "hoge['pre'] = predict[4]\n",
    "hoge.drop(columns = ['YEAR'], inplace = True)\n",
    "len(hoge.query('MAIS3 == 1'))\n",
    "len(hoge.query('MAIS3 == 1 & pre == 0'))\n",
    "print('真値1, 推定値0のデータ')\n",
    "hoge.query('MAIS3 == 1 & pre == 0')\n",
    "print('真値1, 推定値1のデータ')\n",
    "hoge.query('MAIS3 == 1 & pre == 1').sort_values('MAIS', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰で推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_dic = {}\n",
    "coef_df = {}\n",
    "grid_param = {'C': [0.1, 0.5, 1], 'solver': ['liblinear']}\n",
    "for i in range(5):\n",
    "    logi_dic[i] = GridSearchCV(LogisticRegression(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc')\n",
    "    logi_dic[i].fit(train_dummy[i], y_train3)\n",
    "    predict = logi_dic[i].predict(test_dummy[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test3, predict, pos_label = 1)\n",
    "    coef_df[i] = pd.DataFrame({'Feature' : list(train_dummy[i].columns.values), 'coef' : logi_dic[i].best_estimator_.coef_[0]})\n",
    "    coef_df[i]['abs'] = coef_df[i]['coef'].abs()\n",
    "    coef_df[i] = coef_df[i].sort_values('abs', ascending = False).drop('abs', axis = 1)\n",
    "    print('best parameter:', logi_dic[i].best_params_)\n",
    "    print('dic num:', i)\n",
    "    print('test auc: ', auc(fpr, tpr))    \n",
    "    predict = logi_dic[i].predict(train_dummy[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_train3, predict, pos_label = 1)\n",
    "    print('train auc: ', auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    coef_df[1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリング(標準化前)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_train = {}\n",
    "hoge = {}\n",
    "ncl = [[0, 2, 3, 4], [0, 2, 3, 4, 5, 6], [0, 2, 3, 4, 5, 6, 7, 8]]\n",
    "for i in range(5):\n",
    "    trainsample = df_dic[i].query('YEAR < 2015')\n",
    "    trainy = trainsample['MAIS3']\n",
    "    trainX = trainsample.drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    #print('ros')\n",
    "    ros = RandomOverSampler(random_state = 0)\n",
    "    sampling_train['ros_X_{}'.format(i)], sampling_train['ros_y_{}'.format(i)] = ros.fit_sample(trainX, trainy)\n",
    "    #print('rus')\n",
    "    rus = RandomUnderSampler(random_state = 0)\n",
    "    sampling_train['rus_X_{}'.format(i)], sampling_train['rus_y_{}'.format(i)] = rus.fit_sample(trainX, trainy)\n",
    "    #print('tl')\n",
    "    tl = TomekLinks(random_state = 0, ratio = 'majority')\n",
    "    temp_df_X, temp_df_y = tl.fit_sample(trainX, trainy)\n",
    "    sampling_train['tl_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns)\n",
    "    sampling_train['tl_y_{}'.format(i)] = pd.DataFrame(temp_df_y, columns = ['MAIS3'])\n",
    "    #print('cc')\n",
    "    cc = ClusterCentroids(random_state = 0, ratio = 'majority')\n",
    "    temp_df_X, sampling_train['cc_y_{}'.format(i)] = cc.fit_sample(trainX, trainy)\n",
    "    sampling_train['cc_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns).round()\n",
    "    #print('smote')\n",
    "    smote = SMOTE(random_state = 0, ratio = 'minority')\n",
    "    temp_df_X,sampling_train['smote_y_{}'.format(i)] = smote.fit_sample(trainX, trainy)\n",
    "    sampling_train['smote_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns).round() \n",
    "    #print('smt')\n",
    "    smt = SMOTETomek(random_state = 0, ratio = 'auto')\n",
    "    temp_df_X, sampling_train['smt_y_{}'.format(i)] = smt.fit_sample(trainX, trainy)\n",
    "    sampling_train['smt_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns).round()\n",
    "    #print('bsmote')\n",
    "    bsmote = BorderlineSMOTE(random_state = 0)\n",
    "    temp_df_X,sampling_train['bsmote_y_{}'.format(i)] = bsmote.fit_sample(trainX, trainy)\n",
    "    sampling_train['bsmote_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns).round() \n",
    "    #print('adasyn')\n",
    "    adasyn = ADASYN(random_state = 0)\n",
    "    temp_df_X,sampling_train['adasyn_y_{}'.format(i)] = adasyn.fit_sample(trainX, trainy)\n",
    "    sampling_train['adasyn_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns).round()\n",
    "    #print('smotenc')\n",
    "    #ncl = list(range(len(df_dic[i].columns) - 3))#事前にグルーピング\n",
    "    #ncl.pop(1)\n",
    "    #ncl.pop(-1)\n",
    "    #smotenc = SMOTENC(categorical_features = ncl, random_state = 0)\n",
    "    #temp_df_X,sampling_train['smotenc_y_{}'.format(i)] = smotenc.fit_sample(trainX, trainy)\n",
    "    #sampling_train['smotenc_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns)\n",
    "for i in range(5):\n",
    "    for j in ['ros', 'rus']:\n",
    "        sampling_train['{}_X_{}'.format(j, i)] = pd.DataFrame(sampling_train['{}_X_{}'.format(j, i)], columns = df_dic[i].drop(columns = ['YEAR', 'MAIS', 'MAIS3']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tomeklinks + Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_over = {}\n",
    "for i in range(5):\n",
    "    trainy = sampling_train['tl_y_{}'.format(i)]['MAIS3']\n",
    "    trainX = sampling_train['tl_X_{}'.format(i)]\n",
    "    #print('ros')\n",
    "    ros = RandomOverSampler(random_state = 0)\n",
    "    temp_df_X, tl_over['ros_y_{}'.format(i)] = ros.fit_sample(trainX, trainy)\n",
    "    tl_over['ros_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns)\n",
    "    #print('bsmote')\n",
    "    bsmote = BorderlineSMOTE(random_state = 0)\n",
    "    temp_df_X,tl_over['bsmote_y_{}'.format(i)] = bsmote.fit_sample(trainX, trainy)\n",
    "    tl_over['bsmote_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns).round() \n",
    "    #print('adasyn')\n",
    "    adasyn = ADASYN(random_state = 0)\n",
    "    temp_df_X,tl_over['adasyn_y_{}'.format(i)] = adasyn.fit_sample(trainX, trainy)\n",
    "    tl_over['adasyn_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns).round()\n",
    "    #print('smotenc')\n",
    "    #ncl = list(range(len(df_dic[i].columns) - 3))#事前にグルーピング\n",
    "    #ncl.pop(1)\n",
    "    #ncl.pop(-1)\n",
    "    #smotenc = SMOTENC(categorical_features = ncl, random_state = 0)\n",
    "    #temp_df_X,tl_over['smotenc_y_{}'.format(i)] = smotenc.fit_sample(trainX, trainy)\n",
    "    #tl_over['smotenc_X_{}'.format(i)] = pd.DataFrame(temp_df_X, columns = trainX.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリング後グルーピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 5):\n",
    "    for j in ['ros', 'rus', 'tl', 'cc', 'smote', 'smt', 'bsmote', 'adasyn']:#, 'smotenc']:\n",
    "        sampling_train['{}_X_{}'.format(j, i)] = grouping(sampling_train['{}_X_{}'.format(j, i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 5):\n",
    "    for j in ['ros', 'bsmote', 'adasyn']:#, 'smotenc']:\n",
    "        tl_over['{}_X_{}'.format(j, i)] = grouping(tl_over['{}_X_{}'.format(j, i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 5):\n",
    "    for j in ['ros', 'rus', 'tl', 'cc', 'smote', 'smt', 'bsmote', 'adasyn', 'smotenc']:\n",
    "        print(i, j)\n",
    "        np.count_nonzero(sampling_train['{}_y_{}'.format(j, i)] == 0) - np.count_nonzero(sampling_train['{}_y_{}'.format(j, i)] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータをグルーピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 5):\n",
    "    test[i] = grouping(test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリングした訓練データを用いてランダムフォレストで推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_forest = pd.DataFrame(index = range(45),columns = ['サンプリング方法', 'データ番号', 'test auc', 'test accuracy', 'train auc', 'train accuracy', 'auc diff', 'best param']) \n",
    "f_imp_dic = {}\n",
    "count = 0\n",
    "grid_param = {'max_depth': [5], 'min_samples_split': [2], 'min_samples_leaf': [2], 'n_estimators': [500]}\n",
    "for j in ['ros', 'rus', 'tl', 'smote', 'smt', 'bsmote', 'adasyn', 'cc']:#, 'smotenc']:\n",
    "    for i in range(5):\n",
    "        forest = GridSearchCV(RandomForestClassifier(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc') \n",
    "        if j == 'tl':\n",
    "            forest = GridSearchCV(RandomForestClassifier(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc') \n",
    "        forest.fit(sampling_train['{}_X_{}'.format(j, i)], sampling_train['{}_y_{}'.format(j, i)])\n",
    "        pred = forest.best_estimator_.predict(test[i])\n",
    "        fpr, tpr, thresholds = roc_curve(y_test3, pred, pos_label = 1)\n",
    "        test_auc = auc(fpr, tpr)\n",
    "        resample_forest['サンプリング方法'][count] = j\n",
    "        resample_forest['データ番号'][count] = i\n",
    "        resample_forest['best param'][count] = forest.best_params_\n",
    "        resample_forest['test auc'][count] = test_auc\n",
    "        resample_forest['test accuracy'][count] = accuracy_score(y_test3, pred) * 100\n",
    "        pred = forest.best_estimator_.predict(sampling_train['{}_X_{}'.format(j, i)])\n",
    "        fpr, tpr, thresholds = roc_curve(sampling_train['{}_y_{}'.format(j, i)], pred, pos_label = 1)\n",
    "        train_auc = auc(fpr, tpr)\n",
    "        resample_forest['train auc'][count] = train_auc\n",
    "        resample_forest['train accuracy'][count] = accuracy_score(sampling_train['{}_y_{}'.format(j, i)], pred) * 100\n",
    "        resample_forest['auc diff'][count] = train_auc - test_auc\n",
    "        f_imp_dic['{}_{}'.format(j, i)] = pd.DataFrame(index = range(len(sampling_train['{}_X_{}'.format(j, i)].columns)))\n",
    "        imp = forest.best_estimator_.feature_importances_\n",
    "        for k, col in enumerate(sampling_train['{}_X_{}'.format(j, i)].columns):\n",
    "            f_imp_dic['{}_{}'.format(j, i)].at[k, 'column'] = col\n",
    "            f_imp_dic['{}_{}'.format(j, i)].at[k, 'importance'] = imp[k]\n",
    "        f_imp_dic['{}_{}'.format(j, i)] = f_imp_dic['{}_{}'.format(j, i)].sort_values('importance', ascending = False)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in ['ros', 'rus', 'tl', 'smote', 'smt', 'bsmote', 'adasyn', 'cc']:#, 'smotenc']:\n",
    "    for i in range(4, 5):\n",
    "        i, j\n",
    "        f_imp_dic['{}_{}'.format(j, i)].head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tomeklinks + Oversamplingを推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_resample_forest = pd.DataFrame(index = range(45),columns = ['サンプリング方法', 'データ番号', 'test auc', 'test accuracy', 'train auc', 'train accuracy', 'auc diff', 'best param'])\n",
    "count = 0\n",
    "f_imp_dic = {}\n",
    "grid_param = {'max_depth': [5], 'min_samples_split': [2], 'min_samples_leaf': [2], 'n_estimators': [500]}\n",
    "for j in ['ros', 'bsmote', 'adasyn']:#, 'smotenc']:\n",
    "    for i in range(5):\n",
    "        forest = GridSearchCV(RandomForestClassifier(random_state = 0), grid_param, cv = 5, scoring = 'roc_auc') \n",
    "        if j == 'tl':\n",
    "            forest = GridSearchCV(RandomForestClassifier(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc') \n",
    "        forest.fit(tl_over['{}_X_{}'.format(j, i)], tl_over['{}_y_{}'.format(j, i)])\n",
    "        pred = forest.best_estimator_.predict(test[i])\n",
    "        fpr, tpr, thresholds = roc_curve(y_test3, pred, pos_label = 1)\n",
    "        test_auc = auc(fpr, tpr)\n",
    "        tl_resample_forest['サンプリング方法'][count] = j\n",
    "        tl_resample_forest['データ番号'][count] = i\n",
    "        tl_resample_forest['best param'][count] = forest.best_params_\n",
    "        tl_resample_forest['test auc'][count] = test_auc\n",
    "        tl_resample_forest['test accuracy'][count] = accuracy_score(y_test3, pred) * 100\n",
    "        pred = forest.best_estimator_.predict(tl_over['{}_X_{}'.format(j, i)])\n",
    "        fpr, tpr, thresholds = roc_curve(tl_over['{}_y_{}'.format(j, i)], pred, pos_label = 1)\n",
    "        train_auc = auc(fpr, tpr)\n",
    "        tl_resample_forest['train auc'][count] = train_auc\n",
    "        tl_resample_forest['train accuracy'][count] = accuracy_score(tl_over['{}_y_{}'.format(j, i)], pred) * 100\n",
    "        tl_resample_forest['auc diff'][count] = train_auc - test_auc\n",
    "        f_imp_dic['{}_{}'.format(j, i)] = pd.DataFrame(index = range(len(sampling_train['{}_X_{}'.format(j, i)].columns)))\n",
    "        imp = forest.best_estimator_.feature_importances_\n",
    "        for k, col in enumerate(sampling_train['{}_X_{}'.format(j, i)].columns):\n",
    "            f_imp_dic['{}_{}'.format(j, i)].at[k, 'column'] = col\n",
    "            f_imp_dic['{}_{}'.format(j, i)].at[k, 'importance'] = imp[k]\n",
    "        f_imp_dic['{}_{}'.format(j, i)] = f_imp_dic['{}_{}'.format(j, i)].sort_values('importance', ascending = False)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in ['ros', 'bsmote', 'adasyn']:#, 'smotenc']:\n",
    "    for i in range(4, 5):\n",
    "        i, j\n",
    "        f_imp_dic['{}_{}'.format(j, i)].head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリそのまま使用\n",
    "resample_forest = resample_forest.sort_values(['サンプリング方法', 'test auc'], ascending = False)\n",
    "resample_forest.head(len(resample_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomeklinks + Over sampling\n",
    "tl_resample_forest = tl_resample_forest.sort_values(['サンプリング方法', 'test auc'], ascending = False)\n",
    "tl_resample_forest.head(len(resample_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampling_train['tl_y_3'].query('MAIS3 == 0'))\n",
    "len(sampling_train['tl_y_4'].query('MAIS3 == 0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交差検定出力用リサンプリング\n",
    "欠損値補完していないデータを入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_cv = {} #データ番号_cv番号_リサンプリング手法orテスト　を鍵にしてdfを格納\n",
    "cv = 5\n",
    "for i in range(5):\n",
    "    t_df = df_dic[i].query('MAIS3 == 1').drop(columns = ['YEAR', 'MAIS'])\n",
    "    t_len = len(t_df)\n",
    "    t_cv = t_len // cv\n",
    "    t_mod = t_len % cv\n",
    "    f_df = df_dic[i].query('MAIS3 == 0').drop(columns = ['YEAR', 'MAIS'])\n",
    "    f_len = len(f_df)\n",
    "    f_cv = f_len // cv\n",
    "    f_mod = f_len % cv\n",
    "    for j in range(cv):\n",
    "        if t_mod != 0:\n",
    "            t_cv_df = t_df.sample(n = t_cv + 1, random_state = 0)\n",
    "            t_mod -= 1\n",
    "        else:\n",
    "            t_cv_df = t_df.sample(n = t_cv, random_state = 0)\n",
    "        t_df = t_df.drop(t_cv_df.index)\n",
    "        if f_mod != 0:\n",
    "            f_cv_df = f_df.sample(n = f_cv + 1, random_state = 0)\n",
    "            f_mod -= 1\n",
    "        else:\n",
    "            f_cv_df = f_df.sample(n = f_cv, random_state = 0)\n",
    "        f_df = f_df.drop(f_cv_df.index)\n",
    "        sampling_cv['{}_{}_test'.format(i, j)] = pd.concat([t_cv_df, f_cv_df])\n",
    "for i in range(5):\n",
    "    for j in range(cv):\n",
    "        sampling_cv['{}_{}_test_fillna'.format(i, j)] = sampling_cv['{}_{}_test'.format(i, j)]\n",
    "        cv_list = list(range(cv))\n",
    "        cv_list.remove(j)\n",
    "        df = sampling_cv['{}_{}_test'.format(i, cv_list.pop(0))]\n",
    "        for combine in cv_list:\n",
    "            df = pd.concat([df, sampling_cv['{}_{}_test'.format(i, combine)]])\n",
    "        for k in list(df.drop(columns = 'MAIS3').columns):\n",
    "            med = df[k].median()\n",
    "            df[k] = df[k].fillna(med)\n",
    "            sampling_cv['{}_{}_test_fillna'.format(i, j)][k] = sampling_cv['{}_{}_test_fillna'.format(i, j)][k].fillna(med)\n",
    "        trainy = df['MAIS3']\n",
    "        trainX = df.drop(columns = ['MAIS3'])\n",
    "        \n",
    "        ros = RandomOverSampler(random_state = 0)\n",
    "        bsmote = BorderlineSMOTE(random_state = 0)\n",
    "        adasyn = ADASYN(random_state = 0)\n",
    "        rus = RandomUnderSampler(random_state = 0)\n",
    "        cc = ClusterCentroids(random_state = 0)\n",
    "        smote = SMOTE(random_state = 0)\n",
    "        smt = SMOTETomek(random_state = 0)\n",
    "        tl = TomekLinks(random_state = 0)\n",
    "        \n",
    "        resample_list = [ros, bsmote, adasyn, rus, cc, smote, smt, tl, ros, bsmote, adasyn]\n",
    "        resample_name = ['ros', 'bsmote', 'adasyn', 'rus', 'cc', 'smote', 'smt', 'tl', 'tlros', 'tlbsmote', 'tladasyn']\n",
    "        for l, sampling_method in enumerate(resample_list):\n",
    "            temp_df_X, temp_df_y = sampling_method.fit_sample(trainX, trainy.values.ravel())\n",
    "            if resample_name[l] in ['ros', 'rus', 'tlros', 'tl']:\n",
    "                temp_df_X, temp_df_y = pd.DataFrame(temp_df_X, columns = trainX.columns), pd.DataFrame(temp_df_y, columns = ['MAIS3'])\n",
    "            else:\n",
    "                temp_df_X, temp_df_y = pd.DataFrame(temp_df_X, columns = trainX.columns).round(), pd.DataFrame(temp_df_y, columns = ['MAIS3'])\n",
    "            sampling_cv['{}_{}_{}'.format(i, j, resample_name[l])] = pd.concat([temp_df_X, temp_df_y], axis = 1)\n",
    "            if resample_name[l] == 'tl':\n",
    "                trainX, trainy = temp_df_X, temp_df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリング結果を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(path, 'Resampling_data'), exist_ok = True)\n",
    "for i in range(5):\n",
    "    for j in range(cv):\n",
    "        sampling_cv['{}_{}_test_fillna'.format(i, j)] = sampling_cv['{}_{}_test_fillna'.format(i, j)].astype('int')\n",
    "        sampling_cv['{}_{}_test_fillna'.format(i, j)].to_csv(os.path.join(path, 'Resampling_data', '{}_{}_test_fillna.csv'.format(i, j)), index = False)\n",
    "        for k in ['ros', 'rus', 'cc', 'smote', 'smt', 'bsmote', 'adasyn', 'tlros', 'tlbsmote', 'tladasyn']:\n",
    "            sampling_cv['{}_{}_{}'.format(i, j, k)] = sampling_cv['{}_{}_{}'.format(i, j, k)].astype('int')\n",
    "            sampling_cv['{}_{}_{}'.format(i, j, k)].to_csv(os.path.join(path, 'Resampling_data', '{}_{}_{}.csv'.format(i, j, k)), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(path, 'Resampling_data'), exist_ok = True)\n",
    "for i in range(5):\n",
    "    for j in range(cv):\n",
    "        sampling_cv['{}_{}_test_fillna'.format(i, j)] = sampling_cv['{}_{}_test_fillna'.format(i, j)].astype('int')\n",
    "        temp_df = sampling_cv['{}_{}_test_fillna'.format(i, j)].drop(columns = 'MAIS3')\n",
    "        temp_df['MAIS3'] = np.nan\n",
    "        temp_df.to_csv(os.path.join(path, 'Resampling_data', '{}_{}_test_bnet.csv'.format(i, j)), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリング結果をcsvから読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_method = ['ros', 'rus', 'cc', 'smote', 'smt', 'bsmote', 'adasyn', 'tlros', 'tlbsmote', 'tladasyn']\n",
    "path = os.path.dirname(os.path.abspath('__file__'))\n",
    "for i in range(5):\n",
    "    for k in sampling_method:\n",
    "        sampling_cv['{}_{}_test_fillna'.format(i, j)]  = pd.read_csv(os.path.join(path, 'Resampling_data', '{}_{}_test_fillna.csv'.format(i, j)))\n",
    "        for j in range(cv):\n",
    "               sampling_cv['{}_{}_{}'.format(i, j, k)] = pd.read_csv(os.path.join(path, 'Resampling_data', '{}_{}_{}.csv'.format(i, j, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ランダムフォレストで交差検定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_method = ['ros', 'rus', 'cc', 'smote', 'smt', 'bsmote', 'adasyn', 'tlros', 'tlbsmote', 'tladasyn']\n",
    "resample_forest_cv = pd.DataFrame(index = range(5 * cv * len(sampling_method)),columns = ['サンプリング方法', 'データ番号', 'CV', 'MCC', 'PR-AUC',  'ROC-AUC', 'F', 'ACCURACY', 'RECALL', 'PRECISION'])\n",
    "resample_forest_ave = pd.DataFrame(index = range(5 * len(sampling_method)),columns = ['サンプリング方法', 'データ番号', 'MCC', 'PR-AUC',  'ROC-AUC', 'F', 'ACCURACY', 'RECALL', 'PRECISION'])\n",
    "f_imp_dic = {}\n",
    "tree_data = {}\n",
    "count = 0\n",
    "count_ave = 0\n",
    "for i in range(5):\n",
    "    for k in sampling_method:\n",
    "        for j in range(cv):\n",
    "            #学習＆推定\n",
    "            forest = RandomForestClassifier(random_state = 0, max_depth = 5, n_estimators = 500)\n",
    "            forest.fit(sampling_cv['{}_{}_{}'.format(i, j, k)].drop(columns = 'MAIS3'), sampling_cv['{}_{}_{}'.format(i, j, k)]['MAIS3'])\n",
    "            pred = forest.predict(sampling_cv['{}_{}_test_fillna'.format(i, j)].drop(columns = 'MAIS3'))\n",
    "            pred_prob = forest.predict_proba(sampling_cv['{}_{}_test_fillna'.format(i, j)].drop(columns = 'MAIS3'))[:, 1]\n",
    "            #精度\n",
    "            y_true = sampling_cv['{}_{}_test_fillna'.format(i, j)]['MAIS3']\n",
    "            mcc = matthews_corrcoef(y_true, pred)\n",
    "            precision, recall, _ = precision_recall_curve(y_true, pred_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            accuracy = accuracy_score(y_true, pred)\n",
    "            roc_auc = roc_auc_score(y_true, pred_prob)\n",
    "            f_score = f1_score(y_true, pred, pos_label = 1)\n",
    "            recall = recall_score(y_true, pred, pos_label = 1)\n",
    "            precision = precision_score(y_true, pred, pos_label = 1)\n",
    "            assign = [k, i, j, mcc, pr_auc, roc_auc, f_score, accuracy, recall, precision]\n",
    "            for l, col in enumerate(['サンプリング方法', 'データ番号', 'CV', 'MCC', 'PR-AUC',  'ROC-AUC', 'F', 'ACCURACY', 'RECALL', 'PRECISION']):\n",
    "                resample_forest_cv[col][count] = assign[l]\n",
    "            #重要度\n",
    "            f_imp_dic['{}_{}_{}'.format(i, j, k)] = pd.DataFrame(columns = ['column', 'importance'])\n",
    "            imp = forest.feature_importances_\n",
    "            for l, col in enumerate(sampling_cv['{}_{}_{}'.format(i, j,  k)].drop(columns = 'MAIS3').columns):\n",
    "                f_imp_dic['{}_{}_{}'.format(i, j, k)].at[l, 'column'] = col\n",
    "                f_imp_dic['{}_{}_{}'.format(i, j, k)].at[l, 'importance'] = imp[l]\n",
    "            f_imp_dic['{}_{}_{}'.format(i, j, k)] = f_imp_dic['{}_{}_{}'.format(i, j, k)].sort_values('importance', ascending = False)\n",
    "            #モデル保存\n",
    "            tree_data['{}_{}_{}'.format(i, j, k)] = forest.estimators_\n",
    "            count += 1\n",
    "        #精度の平均を保存\n",
    "        temp_df = resample_forest_cv.query('サンプリング方法 == \"{}\" & データ番号 == {}'.format(k, i))\n",
    "        resample_forest_ave['サンプリング方法'][count_ave] = k\n",
    "        resample_forest_ave['データ番号'][count_ave] = i\n",
    "        for col in ['MCC', 'PR-AUC',  'ROC-AUC', 'F', 'ACCURACY', 'RECALL', 'PRECISION']:\n",
    "            resample_forest_ave[col][count_ave] = temp_df[col].mean()\n",
    "        #重要度の平均を保存\n",
    "        df = f_imp_dic['{}_0_{}'.format(i, k)]\n",
    "        for j in range(1, cv):\n",
    "            df = pd.merge(df, f_imp_dic['{}_{}_{}'.format(i, j, k)], on = 'column', how = 'outer')\n",
    "        for col in df.drop(columns = 'column').columns:\n",
    "            df[col] = df[col].astype('float64')\n",
    "        f_imp_dic['{}_{}_ave'.format(i, k)] = pd.concat([df['column'], df.sum(axis = 1).drop(columns = 'column')/5], axis = 1)\n",
    "        count_ave += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の出力\n",
    "平均  \n",
    "- MCC\n",
    "- PR-AUC\n",
    "- F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_forest_cv.head(len(resample_forest_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_forest_ave.sort_values('MCC', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_forest_ave.sort_values('PR-AUC', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_forest_ave.sort_values('F', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重要度の出力\n",
    "cv毎の重要度を平均したものを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in sampling_method:\n",
    "        i, j\n",
    "        f_imp_dic['{}_{}_ave'.format(i, j)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作成した決定木の出力\n",
    "cv = 0の0, 5, -1番目の決定木を出力  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = 'ros'\n",
    "data_num = 1\n",
    "for i in [0, 5, -1]:\n",
    "    tree_viz = dtreeviz(tree_data['{}_{}_{}'.format(data_num, 0, sampling)][i], \n",
    "                        sampling_cv['{}_{}_{}'.format(data_num, 0, sampling)].drop(columns = 'MAIS3'), \n",
    "                        sampling_cv['{}_{}_{}'.format(data_num, 0, sampling)]['MAIS3'], \n",
    "                        feature_names = sampling_cv['{}_{}_{}'.format(data_num, 0, sampling)].drop(columns = 'MAIS3').columns, \n",
    "                        target_name = 'MAIS3', \n",
    "                        class_names = [0, 1])\n",
    "    #tree_viz.view()\n",
    "    display(tree_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用した特徴量による精度比較\n",
    "各リサンプリング手法毎に精度でソートし，各データが何位を取ったか数える  \n",
    "例:ROSについて精度でソートするとデータ4が最も精度が高い  \n",
    "       　→データ4の順位1に+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = 'ROC-AUC'\n",
    "data_df =  pd.DataFrame(index = ['順位' + '{}'.format(i) for i in range(1, 6)],  columns = ['データ' + '{}'.format(i) for i in range(5)])\n",
    "data_df.fillna(0, inplace = True)\n",
    "for i in sampling_method:\n",
    "    temp_df = resample_forest_ave.query('サンプリング方法 == \"{}\"'.format(i))\n",
    "    temp_df = temp_df.sort_values(sort, ascending = False)\n",
    "    for j in range(5):\n",
    "        num = temp_df.iloc[j, 1]\n",
    "        data_df.iloc[j, num] += 1\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不均衡データにも使える3種の評価手法での合計\n",
    "MCC, F値, PR-AUCを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =  pd.DataFrame(index = ['順位' + '{}'.format(i) for i in range(1, 6)],  columns = ['データ' + '{}'.format(i) for i in range(5)])\n",
    "data_df.fillna(0, inplace = True)\n",
    "for sort in ['F', 'MCC', 'PR-AUC']:\n",
    "    for i in sampling_method:\n",
    "        temp_df = resample_forest_ave.query('サンプリング方法 == \"{}\"'.format(i))\n",
    "        temp_df = temp_df.sort_values(sort, ascending = False)\n",
    "        for j in range(5):\n",
    "            num = temp_df.iloc[j, 1]\n",
    "            data_df.iloc[j, num] += 1\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリング手法の比較\n",
    "データ0, データ2は精度が出ないので，データ1, 3, 4についてリサンプリング手法を比較する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =  pd.DataFrame(index = ['順位' + '{}'.format(i) for i in range(1, len(sampling_method) + 1)],  columns = sampling_method)\n",
    "data_df.fillna(0, inplace = True)\n",
    "for sort in ['F', 'MCC', 'PR-AUC']:\n",
    "    for i in [1, 3, 4]:\n",
    "        temp_df = resample_forest_ave.query('データ番号 == {}'.format(i))\n",
    "        temp_df = temp_df.sort_values(sort, ascending = False)\n",
    "        for j in range(len(sampling_method)):\n",
    "            method_name = temp_df.iloc[j, 0]\n",
    "            data_df[method_name][j] += 1\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =  pd.DataFrame(index = ['順位' + '{}'.format(i) for i in range(1, len(sampling_method) + 1)],  columns = sampling_method)\n",
    "data_df.fillna(0, inplace = True)\n",
    "#for sort in ['F', 'MCC', 'PR-AUC']:\n",
    "for sort in ['ROC-AUC']:\n",
    "    for i in [1, 3, 4]:\n",
    "        temp_df = resample_forest_ave.query('データ番号 == {}'.format(i))\n",
    "        temp_df = temp_df.sort_values(sort, ascending = False)\n",
    "        for j in range(len(sampling_method)):\n",
    "            method_name = temp_df.iloc[j, 0]\n",
    "            data_df[method_name][j] += 1\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ベイジアンネットワークでの推定結果の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('読み込みデータ(データ, リサンプリング方法)')\n",
    "bayse_dic = {}\n",
    "for i in range(5):\n",
    "    for k in sampling_method:\n",
    "        for j in range(cv):\n",
    "            bayse_path = os.path.join(path, 'bayse_pred', '{}_{}'.format(i, k), '{}_{}_{}_pred.csv'.format(i, j, k)) \n",
    "            if os.path.exists(bayse_path):\n",
    "                bayse_dic['{}_{}_{}'.format(i, j ,k)] = pd.read_csv(bayse_path)\n",
    "        if os.path.exists(bayse_path):\n",
    "            print('{}, {}'.format(i, k))\n",
    "bayse_key = bayse_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['サンプリング方法', 'データ番号', 'CV', 'MCC', 'PR-AUC',  'ROC-AUC', 'F', 'ACCURACY', 'RECALL', 'PRECISION']\n",
    "bayse_cv = pd.DataFrame(index = [], columns = cols)\n",
    "cols.remove('CV')\n",
    "bayse_ave = pd.DataFrame(index = [], columns = cols)\n",
    "for i in range(5):\n",
    "    for k in sampling_method:\n",
    "        for j in range(cv):\n",
    "            key = '{}_{}_{}'.format(i, j ,k)\n",
    "            if key in bayse_key:\n",
    "                pred = bayse_dic[key]['MAIS3']\n",
    "                pred_prob = bayse_dic[key]['MAIS3.1']\n",
    "                y_true = sampling_cv['{}_{}_test_fillna'.format(i, j)]['MAIS3']\n",
    "                mcc = matthews_corrcoef(y_true, pred)\n",
    "                precision, recall, _ = precision_recall_curve(y_true, pred_prob)\n",
    "                pr_auc = auc(recall, precision)\n",
    "                accuracy = accuracy_score(y_true, pred)\n",
    "                roc_auc = roc_auc_score(y_true, pred_prob)\n",
    "                f_score = f1_score(y_true, pred, pos_label = 1)\n",
    "                recall = recall_score(y_true, pred, pos_label = 1)\n",
    "                precision = precision_score(y_true, pred, pos_label = 1)\n",
    "                record = pd.Series([k, i, j, mcc,  pr_auc, roc_auc, f_score, accuracy, recall, precision], index = bayse_cv.columns)\n",
    "                bayse_cv = bayse_cv.append(record, ignore_index = True)\n",
    "        if key in bayse_key:\n",
    "            temp_df = bayse_cv.query('サンプリング方法 == \"{}\" & データ番号 == {}'.format(k, i))\n",
    "            record = [k, i]\n",
    "            for col in ['MCC', 'PR-AUC',  'ROC-AUC', 'F', 'ACCURACY', 'RECALL', 'PRECISION']:\n",
    "                record.append(temp_df[col].mean())\n",
    "            record = pd.Series(record, index = bayse_ave.columns)\n",
    "            bayse_ave = bayse_ave.append(record, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'MCC'\n",
    "bayse_ave.sort_values('MCC', ascending = False).query('データ番号 == 3')\n",
    "bayse_ave.sort_values('MCC', ascending = False).query('サンプリング方法 == \"ros\"')\n",
    "'F'\n",
    "bayse_ave.sort_values('F', ascending = False).query('データ番号 == 3')\n",
    "bayse_ave.sort_values('F', ascending = False).query('サンプリング方法 == \"ros\"')\n",
    "'PR-AUC'\n",
    "bayse_ave.sort_values('PR-AUC', ascending = False).query('データ番号 == 3')\n",
    "bayse_ave.sort_values('PR-AUC', ascending = False).query('サンプリング方法 == \"ros\"')\n",
    "'ROC-AUC'\n",
    "bayse_ave.sort_values('ROC-AUC', ascending = False).query('データ番号 == 3')\n",
    "bayse_ave.sort_values('ROC-AUC', ascending = False).query('サンプリング方法 == \"ros\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジスティック回帰で交差検定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリ変数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = ['MANCOLL', 'DVper10', 'BODYG', 'obodyg', 'BAGAVAIL',\n",
    "                       'BAGAVOTH', 'MODELG', 'WGTG', 'owgtg', 'AGEG', \n",
    "                       'BMIGJP', 'PDOFG', 'MAIS3'] + ['SEATROW2', 'SEATLR2', 'SEX']\n",
    "col_val = [list(range(x)) for x in [6, 11, 5, 6, 2, 2, 4, 3, 3, 3, 6, 4, 2]] + [list(range(1, 3)) for _ in range(3)]\n",
    "for i in range(5):\n",
    "    for j in range(cv):\n",
    "        for k in sampling_method:\n",
    "            for l, col in enumerate(col_name):\n",
    "                if col in sampling_cv['{}_{}_{}'.format(i, j, k)].columns:\n",
    "                    sampling_cv['{}_{}_{}'.format(i, j, k)][col] = pd.Categorical(sampling_cv['{}_{}_{}'.format(i, j, k)][col], categories = col_val[l])\n",
    "            if col in sampling_cv['{}_{}_test_fillna'.format(i, j)].columns:\n",
    "                sampling_cv['{}_{}_test_fillna'.format(i, j)][col] = pd.Categorical(sampling_cv['{}_{}_test_fillna'.format(i, j)][col], categories = col_val[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ダミー変数化\n",
    "説明変数の変更，カテゴリ変数の持つ値が変わると書き換える必要あり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dummy = ['MANCOLL', 'BODYG', 'obodyg', 'PDOFG']\n",
    "dummy_val = [list(range(6)), list(range(5)), list(range(6)), list(range(4))]\n",
    "sampling_cv_dummy = {}\n",
    "for i in range(5):\n",
    "    for j in range(cv):\n",
    "            for k in sampling_method:\n",
    "                sampling_cv_dummy['{}_{}_{}'.format(i, j, k)] = pd.get_dummies(sampling_cv['{}_{}_{}'.format(i, j, k)], drop_first = True, columns = [x for x in get_dummy if x  in list(sampling_cv['{}_{}_{}'.format(i, j, k)].columns)])\n",
    "            sampling_cv_dummy['{}_{}_test_fillna'.format(i, j)] = pd.get_dummies(sampling_cv['{}_{}_test_fillna'.format(i, j)], drop_first = True, columns = [x for x in get_dummy if x  in list(sampling_cv['{}_{}_test_fillna'.format(i, j)].columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in range(cv):\n",
    "        for k in sampling_method:\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(sampling_cv_dummy['{}_{}_{}'.format(i, j, k)])\n",
    "            col_order = sampling_cv_dummy['{}_{}_{}'.format(i, j, k)].columns\n",
    "            sampling_cv_dummy['{}_{}_{}'.format(i, j, k)] = pd.DataFrame(mms.transform(sampling_cv_dummy['{}_{}_{}'.format(i, j, k)]), columns = col_order)\n",
    "            sampling_cv_dummy['{}_{}_{}_test'.format(i, j, k)] = pd.DataFrame(mms.transform(sampling_cv_dummy['{}_{}_test_fillna'.format(i, j, k)][col_order]), columns = col_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰で推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic回帰パラメータ候補 C: 0.1, 0.5, 1\n",
    "cols = ['サンプリング方法', 'データ番号', 'CV', 'MCC', 'PR-AUC',  'ROC-AUC', 'F', 'ACCURACY', 'RECALL', 'PRECISION']\n",
    "logi_cv = pd.DataFrame(index = [], columns = cols)\n",
    "cols.remove('CV')\n",
    "logi_ave = pd.DataFrame(index = [], columns = cols)\n",
    "logi_coef = {}\n",
    "for i in range(5):\n",
    "    for k in sampling_method:\n",
    "        coef_df = pd.DataFrame(index = [], columns = ['CV'] + sampling_cv_dummy['{}_{}_{}'.format(i, j, k)].drop(columns = 'MAIS3').columns)\n",
    "        i, k\n",
    "        for j in range(cv):\n",
    "            logi = LogisticRegression(random_state = 0, C = 1, solver = 'liblinear')\n",
    "            logi.fit(sampling_cv_dummy['{}_{}_{}'.format(i, j, k)].drop(columns = 'MAIS3'), sampling_cv_dummy['{}_{}_{}'.format(i, j, k)]['MAIS3'])\n",
    "            pred = logi.predict(sampling_cv_dummy['{}_{}_{}_test'.format(i, j, k)].drop(columns = 'MAIS3'))\n",
    "            pred_prob = logi.predict_proba(sampling_cv_dummy['{}_{}_{}_test'.format(i, j, k)].drop(columns = 'MAIS3'))[:, 1]\n",
    "            y_true = sampling_cv_dummy['{}_{}_{}_test'.format(i, j, k)]['MAIS3']\n",
    "            #精度評価\n",
    "            mcc = matthews_corrcoef(y_true, pred)\n",
    "            precision, recall, _ = precision_recall_curve(y_true, pred_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            accuracy = accuracy_score(y_true, pred)\n",
    "            roc_auc = roc_auc_score(y_true, pred_prob)\n",
    "            f_score = f1_score(y_true, pred, pos_label = 1)\n",
    "            recall = recall_score(y_true, pred, pos_label = 1)\n",
    "            precision = precision_score(y_true, pred, pos_label = 1)\n",
    "            record = pd.Series([k, i, j, mcc,  pr_auc, roc_auc, f_score, accuracy, recall, precision], index = logi_cv.columns)\n",
    "            logi_cv = logi_cv.append(record, ignore_index = True)\n",
    "            #係数\n",
    "            record = pd.Series([j] + logi.coef_[0], index = ['CV'] + sampling_cv_dummy['{}_{}_{}'.format(i, j, k)].drop(columns = 'MAIS3').columns)\n",
    "            coef_df = coef_df.append(record, ignore_index = True)\n",
    "        logi_coef['{}_{}'.format(i, k)] = coef_df\n",
    "        logi_coef['{}_{}_ave'.format(i, k)] = pd.DataFrame(coef_df.mean())\n",
    "        #精度平均\n",
    "        temp_df = logi_cv.query('サンプリング方法 == \"{}\" & データ番号 == {}'.format(k, i))\n",
    "        record = [k, i]\n",
    "        for col in ['MCC', 'PR-AUC',  'ROC-AUC', 'F', 'ACCURACY', 'RECALL', 'PRECISION']:\n",
    "            record.append(temp_df[col].mean())\n",
    "        record = pd.Series(record, index = bayse_ave.columns)\n",
    "        logi_ave = logi_ave.append(record, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推定結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_ave.sort_values('F', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for k in sampling_method:\n",
    "        i, k\n",
    "        logi_coef['{}_{}_ave'.format(i, k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリング(標準化後)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_train = {}\n",
    "for i in range(5):\n",
    "    trainsample = df_dic[i].query('YEAR < 2015')\n",
    "    trainy = trainsample['MAIS3']\n",
    "    trainX = trainsample.drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    #print('ros')\n",
    "    ros = RandomOverSampler(random_state = 0)\n",
    "    sampling_train['ros_X_{}'.format(i)], sampling_train['ros_y_{}'.format(i)] = ros.fit_sample(trainX, trainy)\n",
    "    sampling_train['ros_X_{}'.format(i)] = pd.DataFrame(sampling_train['ros_X_{}'.format(i)], columns = [x for x in trainX.columns])\n",
    "    sampling_train['ros_X_{}'.format(i)] = pd.get_dummies(sampling_train['ros_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['ros_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('rus')\n",
    "    rus = RandomUnderSampler(random_state = 0)\n",
    "    sampling_train['rus_X_{}'.format(i)], sampling_train['rus_y_{}'.format(i)] = rus.fit_sample(trainX, trainy)\n",
    "    sampling_train['rus_X_{}'.format(i)] = pd.DataFrame(sampling_train['rus_X_{}'.format(i)], columns = [x for x in trainX.columns])\n",
    "    sampling_train['rus_X_{}'.format(i)] = pd.get_dummies(sampling_train['rus_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['rus_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('tl')\n",
    "    tl = TomekLinks(random_state = 0, ratio = 'majority')\n",
    "    sampling_train['tl_X_{}'.format(i)], sampling_train['tl_y_{}'.format(i)] = tl.fit_sample(trainX, trainy)\n",
    "    sampling_train['tl_X_{}'.format(i)] = pd.DataFrame(sampling_train['tl_X_{}'.format(i)], columns = [x for x in trainX.columns])\n",
    "    sampling_train['tl_X_{}'.format(i)] = pd.get_dummies(sampling_train['tl_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['tl_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('cc')\n",
    "    cc = ClusterCentroids(random_state = 0, ratio = 'majority')\n",
    "    sampling_train['cc_X_{}'.format(i)], sampling_train['cc_y_{}'.format(i)] = cc.fit_sample(trainX, trainy)\n",
    "    sampling_train['cc_X_{}'.format(i)] = pd.DataFrame(sampling_train['cc_X_{}'.format(i)], columns = [x for x in trainX.columns])\n",
    "    sampling_train['cc_X_{}'.format(i)] = sampling_train['cc_X_{}'.format(i)][[x for x in sampling_train['cc_X_{}'.format(i)].columns if x not in stdcol]].round() \n",
    "    sampling_train['cc_X_{}'.format(i)] = pd.get_dummies(sampling_train['cc_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['cc_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('smote')\n",
    "    smote = SMOTE(random_state = 0, ratio = 'minority')\n",
    "    temp_df_X,sampling_train['smote_y_{}'.format(i)] = smote.fit_sample(trainX, trainy)\n",
    "    temp_df_X = pd.DataFrame(temp_df_X, columns = [x for x in trainX.columns])\n",
    "    sampling_train['smote_X_{}'.format(i)] = temp_df_X[[x for x in temp_df_X.columns if x not in stdcol]].round() \n",
    "    sampling_train['smote_X_{}'.format(i)] = pd.get_dummies(sampling_train['smote_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['smote_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('smt')\n",
    "    smt = SMOTETomek(random_state = 0, ratio = 'auto')\n",
    "    temp_df_X, sampling_train['smt_y_{}'.format(i)] = smt.fit_sample(trainX, trainy)\n",
    "    temp_df_X = pd.DataFrame(temp_df_X, columns = [x for x in trainX.columns])\n",
    "    sampling_train['smt_X_{}'.format(i)] = temp_df_X[[x for x in temp_df_X.columns if x not in stdcol]].round() \n",
    "    sampling_train['smt_X_{}'.format(i)] = pd.get_dummies(sampling_train['smt_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['smt_X_{}'.format(i)].columns) if x not in not_dummy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMで推定(重い割にそこまで精度は出ない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_dic = {}\n",
    "grid_param = {'C': [100, 500], 'gamma': [0.001, 0.01]}\n",
    "for i in range(5):\n",
    "    svm_dic[i] = GridSearchCV(svm.SVC(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc')\n",
    "    svm_dic[i].fit(train_dummy[i], y_train3)\n",
    "    predict = svm_dic[i].predict(test_dummy[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test3, predict, pos_label = 1)\n",
    "    print('best parameter:', svm_dic[i].best_params_)\n",
    "    print('dic num:', i)\n",
    "    print('test auc: ', auc(fpr, tpr))    \n",
    "    predict = svm_dic[i].predict(train_dummy[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_train3, predict, pos_label = 1)\n",
    "    print('train auc: ', auc(fpr, tpr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
