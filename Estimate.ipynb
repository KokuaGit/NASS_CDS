{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリの読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, ClusterCentroids, NearMiss\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import  confusion_matrix, classification_report, roc_curve, auc, accuracy_score, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from IPython import embed\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.options.display.float_format = '{:.4g}'.format\n",
    "class pycolor:\n",
    "    BLACK = '\\033[30m'\n",
    "    RED = '\\033[31m'\n",
    "    GREEN = '\\033[32m'\n",
    "    YELLOW = '\\033[33m'\n",
    "    BLUE = '\\033[34m'\n",
    "    PURPLE = '\\033[35m'\n",
    "    CYAN = '\\033[36m'\n",
    "    WHITE = '\\033[37m'\n",
    "    END = '\\033[0m'\n",
    "    BOLD = '\\038[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    INVISIBLE = '\\033[08m'\n",
    "    REVERCE = '\\033[07m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.path.abspath('__file__'))\n",
    "df = pd.read_csv(os.path.join(path, 'grouped_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データに含まれるカラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同じ内容のカラムの片方をドロップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['SEATROW', 'SEATLR', 'BMIG', 'PDOF1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'DVper10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'TRAVELSP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NANと同じ意味のデータをNANに置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TRAVELSP'] = df['TRAVELSP'].replace(777, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相関係数の高い特徴量をドロップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic  = {}\n",
    "count = 0\n",
    "for i, col1 in enumerate([['MODELG', 'SEATROW2'], ['BAGAVAIL', 'BAGAVOTH']]):\n",
    "    for j, col2 in enumerate([['BODYG', 'obodyg'], ['WGTG',  'owgtg']]):\n",
    "        df_dic[count] = df.drop(columns = col1 + col2)\n",
    "        #df_dic[count] = df.drop(columns = 'POS_DOF')\n",
    "        print(count)\n",
    "        print('remain : ', list(df_dic[count].columns))\n",
    "        print('drop : ', col1 + col2)\n",
    "        count += 1\n",
    "df_dic[count] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdcol = ['SPLIMIT', 'TRAVELSP', 'DVTOTAL']\n",
    "for i in range(5):\n",
    "    for j in stdcol:\n",
    "        std = StandardScaler()\n",
    "        df_std = df_dic[i].query('YEAR < 2015')\n",
    "        if j in df_std.columns:\n",
    "            null_val = df_std[j].isnull()\n",
    "            std.fit(df_std.loc[~null_val, [j]])\n",
    "            null_val = df_dic[i][j].isnull()\n",
    "            df_dic[i].loc[~null_val, [j]] = std.transform(df_dic[i].loc[~null_val, [j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中央値で欠損値補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in list(df_dic[i].columns):\n",
    "        df_med = df_dic[i].query('YEAR < 2015')\n",
    "        med = df_med[j].median()\n",
    "        if df_dic[i][j].isnull().any():\n",
    "            print(j, ':', med)\n",
    "        df_dic[i][j] = df_dic[i][j].fillna(med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値補完の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('欠損値数：', df_dic[i].isnull().any().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ダミー変数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dic = {}\n",
    "not_dummy = ['YEAR', 'MAIS', 'MAIS3', 'DVTOTAL', 'TRAVELSP']\n",
    "for i in range(5):\n",
    "    dummy_dic[i] = pd.get_dummies(df_dic[i], drop_first = False, columns = [x for x in list(df_dic[i].columns) if x not in not_dummy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータと訓練データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummy =  {}\n",
    "train = {}\n",
    "test_dummy =  {}\n",
    "test = {}\n",
    "explanatory = {}\n",
    "y_train = df.query('YEAR < 2015')['MAIS']\n",
    "y_train3 = df.query('YEAR < 2015')['MAIS3']\n",
    "y_test = df.query('YEAR == 2015')['MAIS']\n",
    "y_test3 = df.query('YEAR == 2015')['MAIS3']\n",
    "objective = df['MAIS']\n",
    "objective3 = df['MAIS3']\n",
    "for i in range(5):\n",
    "    train_dummy[i] = dummy_dic[i].query('YEAR < 2015').drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    train[i] = df_dic[i].query('YEAR < 2015').drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    test_dummy[i] = dummy_dic[i].query('YEAR == 2015').drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    test[i] = df_dic[i].query('YEAR == 2015').drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    explanatory[i] = df_dic[i].drop(columns = ['YEAR', 'MAIS', 'MAIS3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ランダムフォレストで推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_dic = {}\n",
    "gs_result = {}\n",
    "predict = {}#テストデータセットの推定結果\n",
    "pred = {}#訓練データでの推定結果\n",
    "f_imp_dic = {}\n",
    "grid_param = {'max_depth': [5], 'min_samples_split': [2], 'min_samples_leaf': [2], 'n_estimators': [500]}\n",
    "for i in range(5):\n",
    "    forest_dic[i] = GridSearchCV(RandomForestClassifier(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc')\n",
    "    forest_dic[i].fit(train[i], y_train3)\n",
    "    predict[i] = forest_dic[i].predict(test[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test3, predict[i], pos_label = 1)\n",
    "    print('best parameter:', forest_dic[i].best_params_)\n",
    "    print('dic num:', i)\n",
    "    print('test auc:', auc(fpr, tpr))\n",
    "    print('test accuracy:', accuracy_score(y_test3, predict[i]) * 100)\n",
    "    pred[i] = forest_dic[i].predict(train[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_train3, pred[i], pos_label = 1)\n",
    "    print('train auc:', auc(fpr, tpr))\n",
    "    print('train accuracy:', accuracy_score(y_train3, pred[i]) * 100)\n",
    "    gs_result[i] = pd.DataFrame.from_dict(forest_dic[i].cv_results_)\n",
    "    f_imp_dic[i] = pd.DataFrame(index = range(len(train[i].columns)))\n",
    "    imp = forest_dic[i].best_estimator_.feature_importances_\n",
    "    for j, col in enumerate(train[i].columns):\n",
    "        f_imp_dic[i].at[j, 'column'] = col\n",
    "        f_imp_dic[i].at[j, 'importance'] = imp[j]\n",
    "    f_imp_dic[i] = f_imp_dic[i].sort_values('importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際のMAIS3+が1で推定結果が0となっているデータの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hoge = df.query('YEAR == 2015')\n",
    "hoge['pre'] = predict[4]\n",
    "hoge.drop(columns = ['YEAR'], inplace = True)\n",
    "len(hoge.query('MAIS3 == 1'))\n",
    "len(hoge.query('MAIS3 == 1 & pre == 0'))\n",
    "print('真値1, 推定値0のデータ')\n",
    "hoge.query('MAIS3 == 1 & pre == 0')\n",
    "print('真値1, 推定値1のデータ')\n",
    "hoge.query('MAIS3 == 1 & pre == 1').sort_values('MAIS', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰で推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logi_dic = {}\n",
    "coef_df = {}\n",
    "grid_param = {'C': [0.1, 0.5, 1], 'solver': ['liblinear']}\n",
    "for i in range(5):\n",
    "    logi_dic[i] = GridSearchCV(LogisticRegression(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc')\n",
    "    logi_dic[i].fit(train_dummy[i], y_train3)\n",
    "    predict = logi_dic[i].predict(test_dummy[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test3, predict, pos_label = 1)\n",
    "    coef_df[i] = pd.DataFrame({'Feature' : list(train_dummy[i].columns.values), 'coef' : logi_dic[i].best_estimator_.coef_[0]})\n",
    "    coef_df[i]['abs'] = coef_df[i]['coef'].abs()\n",
    "    coef_df[i] = coef_df[i].sort_values('abs', ascending = False).drop('abs', axis = 1)\n",
    "    print('best parameter:', logi_dic[i].best_params_)\n",
    "    print('dic num:', i)\n",
    "    print('test auc: ', auc(fpr, tpr))    \n",
    "    predict = logi_dic[i].predict(train_dummy[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_train3, predict, pos_label = 1)\n",
    "    print('train auc: ', auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    coef_df[1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリング(標準化前)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sampling_train = {}\n",
    "for i in range(5):\n",
    "    trainsample = df_dic[i].query('YEAR < 2015')\n",
    "    trainy = trainsample['MAIS3']\n",
    "    trainX = trainsample.drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    #print('ros')\n",
    "    ros = RandomOverSampler(random_state = 0)\n",
    "    sampling_train['ros_X_{}'.format(i)], sampling_train['ros_y_{}'.format(i)] = ros.fit_sample(trainX, trainy)\n",
    "    #print('rus')\n",
    "    rus = RandomUnderSampler(random_state = 0)\n",
    "    sampling_train['rus_X_{}'.format(i)], sampling_train['rus_y_{}'.format(i)] = rus.fit_sample(trainX, trainy)\n",
    "    #print('tl')\n",
    "    tl = TomekLinks(random_state = 0, ratio = 'majority')\n",
    "    sampling_train['tl_X_{}'.format(i)], sampling_train['tl_y_{}'.format(i)] = tl.fit_sample(trainX, trainy)\n",
    "    #print('cc')\n",
    "    cc = ClusterCentroids(random_state = 0, ratio = 'majority')\n",
    "    temp_df_X, sampling_train['cc_y_{}'.format(i)] = cc.fit_sample(trainX, trainy)\n",
    "    sampling_train['cc_X_{}'.format(i)] = pd.DataFrame(temp_df_X).round()\n",
    "    #print('smote')\n",
    "    smote = SMOTE(random_state = 0, ratio = 'minority')\n",
    "    temp_df_X,sampling_train['smote_y_{}'.format(i)] = smote.fit_sample(trainX, trainy)\n",
    "    sampling_train['smote_X_{}'.format(i)] = pd.DataFrame(temp_df_X).round() \n",
    "    #print('smt')\n",
    "    smt = SMOTETomek(random_state = 0, ratio = 'auto')\n",
    "    temp_df_X, sampling_train['smt_y_{}'.format(i)] = smt.fit_sample(trainX, trainy)\n",
    "    sampling_train['smt_X_{}'.format(i)] = pd.DataFrame(temp_df_X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in ['ros', 'rus', 'tl', 'cc', 'smote', 'smt']:\n",
    "        print(i, j)\n",
    "        np.count_nonzero(sampling_train['{}_y_{}'.format(j, i)] == 0) - np.count_nonzero(sampling_train['{}_y_{}'.format(j, i)] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリングした訓練データを用いてランダムフォレストで推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resample_forest = pd.DataFrame(index = range(30),columns = ['サンプリング方法', 'データ番号', 'test auc', 'test accuracy', 'train auc', 'train accuracy', 'auc diff', 'best param'])\n",
    "count = 0\n",
    "grid_param = {'max_depth': [5], 'min_samples_split': [2], 'min_samples_leaf': [2], 'n_estimators': [500]}\n",
    "for j in ['ros', 'rus', 'tl', 'cc', 'smote', 'smt']:\n",
    "    for i in range(5):\n",
    "        forest = GridSearchCV(RandomForestClassifier(random_state = 0), grid_param, cv = 5, scoring = 'roc_auc') \n",
    "        if j == 'tl':\n",
    "            forest = GridSearchCV(RandomForestClassifier(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc') \n",
    "        forest.fit(sampling_train['{}_X_{}'.format(j, i)], sampling_train['{}_y_{}'.format(j, i)])\n",
    "        pred = forest.best_estimator_.predict(test[i])\n",
    "        fpr, tpr, thresholds = roc_curve(y_test3, pred, pos_label = 1)\n",
    "        test_auc = auc(fpr, tpr)\n",
    "        resample_forest['サンプリング方法'][count] = j\n",
    "        resample_forest['データ番号'][count] = i\n",
    "        resample_forest['best param'][count] = forest.best_params_\n",
    "        resample_forest['test auc'][count] = test_auc\n",
    "        resample_forest['test accuracy'][count] = accuracy_score(y_test3, pred) * 100\n",
    "        pred = forest.best_estimator_.predict(sampling_train['{}_X_{}'.format(j, i)])\n",
    "        fpr, tpr, thresholds = roc_curve(sampling_train['{}_y_{}'.format(j, i)], pred, pos_label = 1)\n",
    "        train_auc = auc(fpr, tpr)\n",
    "        resample_forest['train auc'][count] = train_auc\n",
    "        resample_forest['train accuracy'][count] = accuracy_score(sampling_train['{}_y_{}'.format(j, i)], pred) * 100\n",
    "        resample_forest['auc diff'][count] = train_auc - test_auc\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resample_forest = resample_forest.sort_values(['サンプリング方法', 'test auc'], ascending = False)\n",
    "resample_forest.head(len(resample_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリング(標準化後)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling_train = {}\n",
    "for i in range(5):\n",
    "    trainsample = df_dic[i].query('YEAR < 2015')\n",
    "    trainy = trainsample['MAIS3']\n",
    "    trainX = trainsample.drop(columns = ['YEAR', 'MAIS', 'MAIS3'])\n",
    "    #print('ros')\n",
    "    ros = RandomOverSampler(random_state = 0)\n",
    "    sampling_train['ros_X_{}'.format(i)], sampling_train['ros_y_{}'.format(i)] = ros.fit_sample(trainX, trainy)\n",
    "    sampling_train['ros_X_{}'.format(i)] = pd.DataFrame(sampling_train['ros_X_{}'.format(i)], columns = [x for x in trainX.columns])\n",
    "    sampling_train['ros_X_{}'.format(i)] = pd.get_dummies(sampling_train['ros_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['ros_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('rus')\n",
    "    rus = RandomUnderSampler(random_state = 0)\n",
    "    sampling_train['rus_X_{}'.format(i)], sampling_train['rus_y_{}'.format(i)] = rus.fit_sample(trainX, trainy)\n",
    "    sampling_train['rus_X_{}'.format(i)] = pd.DataFrame(sampling_train['rus_X_{}'.format(i)], columns = [x for x in trainX.columns])\n",
    "    sampling_train['rus_X_{}'.format(i)] = pd.get_dummies(sampling_train['rus_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['rus_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('tl')\n",
    "    tl = TomekLinks(random_state = 0, ratio = 'majority')\n",
    "    sampling_train['tl_X_{}'.format(i)], sampling_train['tl_y_{}'.format(i)] = tl.fit_sample(trainX, trainy)\n",
    "    sampling_train['tl_X_{}'.format(i)] = pd.DataFrame(sampling_train['tl_X_{}'.format(i)], columns = [x for x in trainX.columns])\n",
    "    sampling_train['tl_X_{}'.format(i)] = pd.get_dummies(sampling_train['tl_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['tl_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('cc')\n",
    "    cc = ClusterCentroids(random_state = 0, ratio = 'majority')\n",
    "    sampling_train['cc_X_{}'.format(i)], sampling_train['cc_y_{}'.format(i)] = cc.fit_sample(trainX, trainy)\n",
    "    sampling_train['cc_X_{}'.format(i)] = pd.DataFrame(sampling_train['cc_X_{}'.format(i)], columns = [x for x in trainX.columns])\n",
    "    sampling_train['cc_X_{}'.format(i)] = sampling_train['cc_X_{}'.format(i)][[x for x in sampling_train['cc_X_{}'.format(i)].columns if x not in stdcol]].round() \n",
    "    sampling_train['cc_X_{}'.format(i)] = pd.get_dummies(sampling_train['cc_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['cc_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('smote')\n",
    "    smote = SMOTE(random_state = 0, ratio = 'minority')\n",
    "    temp_df_X,sampling_train['smote_y_{}'.format(i)] = smote.fit_sample(trainX, trainy)\n",
    "    temp_df_X = pd.DataFrame(temp_df_X, columns = [x for x in trainX.columns])\n",
    "    sampling_train['smote_X_{}'.format(i)] = temp_df_X[[x for x in temp_df_X.columns if x not in stdcol]].round() \n",
    "    sampling_train['smote_X_{}'.format(i)] = pd.get_dummies(sampling_train['smote_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['smote_X_{}'.format(i)].columns) if x not in not_dummy])\n",
    "    #print('smt')\n",
    "    smt = SMOTETomek(random_state = 0, ratio = 'auto')\n",
    "    temp_df_X, sampling_train['smt_y_{}'.format(i)] = smt.fit_sample(trainX, trainy)\n",
    "    temp_df_X = pd.DataFrame(temp_df_X, columns = [x for x in trainX.columns])\n",
    "    sampling_train['smt_X_{}'.format(i)] = temp_df_X[[x for x in temp_df_X.columns if x not in stdcol]].round() \n",
    "    sampling_train['smt_X_{}'.format(i)] = pd.get_dummies(sampling_train['smt_X_{}'.format(i)], drop_first = False, columns = [x for x in list(sampling_train['smt_X_{}'.format(i)].columns) if x not in not_dummy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リサンプリングした訓練データを用いてロジスティック回帰で推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resample_logi = pd.DataFrame(index = range(30),columns = ['サンプリング方法', 'データ番号', 'test auc', 'test accuracy', 'train auc', 'train accuracy', 'auc diff', 'best param'])\n",
    "count = 0\n",
    "grid_param = {'C': [0.1, 0.5, 1], 'solver': ['liblinear']}\n",
    "for j  in ['ros', 'rus', 'tl', 'smt', 'cc', 'smote']:\n",
    "    print(j)\n",
    "    for i in range(5):\n",
    "        logi = GridSearchCV(LogisticRegression(random_state = 0), grid_param, cv = 5, scoring = 'roc_auc')\n",
    "        if j == 'tl':\n",
    "            logi = GridSearchCV(LogisticRegression(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc')\n",
    "        logi.fit(sampling_train['{}_X_{}'.format(j, i)], sampling_train['{}_y_{}'.format(j, i)])\n",
    "        pred = logi.best_estimator_.predict(test_dummy[i])\n",
    "        fpr, tpr, thresholds = roc_curve(y_test3, pred, pos_label = 1)\n",
    "        test_auc = auc(fpr, tpr)\n",
    "        resample_logi['サンプリング方法'][count] = j\n",
    "        resample_logi['データ番号'][count] = i\n",
    "        resample_logi['best param'][count] = logi.best_params_\n",
    "        resample_logi['test auc'][count] = test_auc\n",
    "        resample_logi['test accuracy'][count] = accuracy_score(y_test3, pred) * 100\n",
    "        pred = logi.best_estimator_.predict(sampling_train['{}_X_{}'.format(j, i)])\n",
    "        fpr, tpr, thresholds = roc_curve(sampling_train['{}_y_{}'.format(j, i)], pred, pos_label = 1)\n",
    "        train_auc = auc(fpr, tpr)\n",
    "        resample_logi['train auc'][count] = train_auc\n",
    "        resample_logi['train accuracy'][count] = accuracy_score(sampling_train['{}_y_{}'.format(j, i)], pred) * 100\n",
    "        resample_logi['auc diff'][count] = train_auc - test_auc\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resample_logi = resample_logi.sort_values(['サンプリング方法', 'test auc'], ascending = False)\n",
    "resample_logi.head(len(resample_logi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## SVMで推定(重い割にそこまで精度は出ない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_dic = {}\n",
    "grid_param = {'C': [100, 500], 'gamma': [0.001, 0.01]}\n",
    "for i in range(5):\n",
    "    svm_dic[i] = GridSearchCV(svm.SVC(random_state = 0, class_weight = 'balanced'), grid_param, cv = 5, scoring = 'roc_auc')\n",
    "    svm_dic[i].fit(train_dummy[i], y_train3)\n",
    "    predict = svm_dic[i].predict(test_dummy[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test3, predict, pos_label = 1)\n",
    "    print('best parameter:', svm_dic[i].best_params_)\n",
    "    print('dic num:', i)\n",
    "    print('test auc: ', auc(fpr, tpr))    \n",
    "    predict = svm_dic[i].predict(train_dummy[i])\n",
    "    fpr, tpr, thresholds = roc_curve(y_train3, predict, pos_label = 1)\n",
    "    print('train auc: ', auc(fpr, tpr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
